# StarLinker v. 1.4
# a tool for searching YouTube influencers
# (c) StarPets 2025

import os
import re
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Tuple, Optional, Iterable

import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from dotenv import load_dotenv

# =============== PAGE CONFIG ===============
st.set_page_config(page_title="StarLinker", page_icon="üîé", layout="wide")
st.title("üîé StarLinker ‚Äî –ü–æ–∏—Å–∫ –±–ª–æ–≥–µ—Ä–æ–≤ –Ω–∞ YouTube (v1.4)")

# =============== API KEY ===============
load_dotenv()
API_KEY = st.secrets.get("YOUTUBE_API_KEY") or os.getenv("YOUTUBE_API_KEY")

def build_client():
    if not API_KEY:
        return None
    return build("youtube", "v3", developerKey=API_KEY)

# =============== HELPERS ===============
UC_RE = re.compile(r"(UC[0-9A-Za-z_-]{20,})", re.I)

def chunked(seq: List[str], n: int) -> Iterable[List[str]]:
    for i in range(0, len(seq), n):
        yield seq[i:i+n]

def iso_to_dt(s: str) -> datetime:
    return datetime.fromisoformat(s.replace("Z", "+00:00"))

def extract_uc_id(s: str) -> Optional[str]:
    """–î–æ—Å—Ç–∞—ë—Ç UC-id –∏–∑ —Å—Ç—Ä–æ–∫–∏ (–∏–∑ –ª—é–±–æ–π —Ñ–æ—Ä–º—ã URL/—Ç–µ–∫—Å—Ç–∞)."""
    if not isinstance(s, str):
        return None
    s = s.strip()
    m = UC_RE.search(s)
    if m:
        return m.group(1)
    # youtube.com/channel/UC...
    m2 = re.search(r"channel/(UC[0-9A-Za-z_-]{20,})", s, re.I)
    if m2:
        return m2.group(1)
    return None

def extract_handle(s: str, treat_bare_names: bool = False) -> Optional[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç handle –±–µ–∑ @ (—Å—Ç—Ä–æ—á–Ω—ã–µ), –µ—Å–ª–∏ –µ–≥–æ –º–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –∏–∑ —Å—Ç—Ä–æ–∫–∏."""
    if not isinstance(s, str):
        return None
    s0 = s.strip()

    # 1) @handle –≥–¥–µ —É–≥–æ–¥–Ω–æ
    m = re.search(r"@([A-Za-z0-9._-]{3,})", s0)
    if m:
        return m.group(1).lower()

    # 2) youtube.com/@handle
    m = re.search(r"youtube\.com/\@([A-Za-z0-9._-]{3,})", s0, re.I)
    if m:
        return m.group(1).lower()

    # 3) youtube.com/c/<name> –∏–ª–∏ /user/<name> ‚Üí —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ handle-–∫–∞–Ω–¥–∏–¥–∞—Ç
    m = re.search(r"youtube\.com/(?:c|user)/([A-Za-z0-9._-]{3,})", s0, re.I)
    if m:
        return m.group(1).lower()

    # 4) –ø—Ä–æ—Å—Ç–æ —Å–ª–æ–≤–æ (–ø–æ –∂–µ–ª–∞–Ω–∏—é)
    if treat_bare_names:
        token = re.sub(r"\s+", "", s0)
        token = token.split("/")[0].strip(".")
        if re.fullmatch(r"[A-Za-z0-9._-]{3,}", token):
            return token.lower()

    return None

_RESOLVE_CACHE: Dict[str, Optional[str]] = {}

def resolve_handle_to_channel_id(youtube, handle: str) -> Optional[str]:
    """–ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ channel_id –ø–æ handle/–∫–∞—Å—Ç–æ–º–Ω–æ–º—É URL. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç UC-id –∏–ª–∏ None."""
    if handle in _RESOLVE_CACHE:
        return _RESOLVE_CACHE[handle]

    if not youtube:
        _RESOLVE_CACHE[handle] = None
        return None

    try:
        # –ò—â–µ–º –∫–∞–Ω–∞–ª—ã –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é/handle
        sres = youtube.search().list(
            q=handle,
            part="snippet",
            type="channel",
            maxResults=10
        ).execute()
        ch_ids = [it["snippet"]["channelId"] for it in sres.get("items", []) if it.get("snippet")]
        if not ch_ids:
            _RESOLVE_CACHE[handle] = None
            return None

        # –¢—è–Ω–µ–º snippet –∫–∞–Ω–∞–ª–æ–≤, —á—Ç–æ–±—ã —Å–≤–µ—Ä–∏—Ç—å customUrl
        cres = youtube.channels().list(
            part="snippet",
            id=",".join(ch_ids[:50])
        ).execute()

        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ customUrl —Å handle
        for it in cres.get("items", []):
            sn = it.get("snippet", {})
            custom = (sn.get("customUrl") or "").lstrip("@").lower()
            if custom and custom == handle.lower():
                _RESOLVE_CACHE[handle] = it["id"]
                return it["id"]

        # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ –Ω–µ—Ç ‚Äî –≤–æ–∑—å–º—ë–º –ø–µ—Ä–≤—ã–π –∏–∑ –ø–æ–∏—Å–∫–∞ –∫–∞–∫ –Ω–∞–∏–ª—É—á—à–µ–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ
        _RESOLVE_CACHE[handle] = ch_ids[0]
        return ch_ids[0]

    except HttpError:
        _RESOLVE_CACHE[handle] = None
        return None

def normalize_existing_ids_from_df(
    youtube,
    df: pd.DataFrame,
    treat_bare_names: bool,
    do_normalize: bool,
    max_to_normalize: int
) -> Tuple[set, dict]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (existing_ids_set, stats_dict)
    –°–Ω–∞—á–∞–ª–∞ –±–µ—Ä—ë–º –≤—Å–µ UC-id –∏–∑ —è–≤–Ω–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ channel_id –∏ —Å—Å—ã–ª–æ–∫ /channel/UC‚Ä¶,
    –∑–∞—Ç–µ–º –ø–æ –∂–µ–ª–∞–Ω–∏—é –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º handle/custom URL ‚Üí UC-id.
    """
    cols_lower = {c.lower(): c for c in df.columns}
    existing_ids: set = set()
    stats = {"taken_channel_id": 0, "extracted_from_url": 0, "normalized": 0, "unresolved": 0}

    # 1) —è–≤–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü channel_id
    cid_col = None
    for k, orig in cols_lower.items():
        if k == "channel_id" or k.endswith("channel_id") or k == "id":
            cid_col = orig
            break
    if cid_col:
        for v in df[cid_col].dropna():
            uc = extract_uc_id(str(v))
            if uc:
                existing_ids.add(uc)
                stats["taken_channel_id"] += 1

    # 2) —Å—Å—ã–ª–∫–∏ (–°—Å—ã–ª–∫–∞/Link/URL/‚Ä¶)
    link_col = None
    if not cid_col:
        for k, orig in cols_lower.items():
            if any(key in k for key in ["—Å—Å—ã–ª", "link", "url"]):
                link_col = orig
                break
    if link_col:
        for v in df[link_col].dropna():
            s = str(v)
            uc = extract_uc_id(s)
            if uc:
                existing_ids.add(uc)
                stats["extracted_from_url"] += 1

    # 3) –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è handle/custom URL ‚Üí UC-id
    if do_normalize:
        # –°–æ–±–µ—Ä—ë–º –≤—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã-—Å—Ç—Ä–æ–∫–∏
        candidates: List[str] = []
        # –ë–µ—Ä—ë–º –ª–∏–±–æ link-–∫–æ–ª–æ–Ω–∫—É, –ª–∏–±–æ –≤—Å—é —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –Ω–µ—Ç
        source_cols = [cid_col] if cid_col else ([link_col] if link_col else list(df.columns))
        seen_raw = set()
        for c in source_cols:
            if not c:
                continue
            for v in df[c].dropna():
                s = str(v)
                if s in seen_raw:
                    continue
                seen_raw.add(s)
                # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —É–∂–µ UC –Ω–∞–π–¥–µ–Ω
                if extract_uc_id(s):
                    continue
                # –≤—ã–¥–µ–ª—è–µ–º handle
                h = extract_handle(s, treat_bare_names=treat_bare_names)
                if h:
                    candidates.append(h)

        # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –∫–≤–æ—Ç–µ
        to_norm = list(dict.fromkeys(candidates))[: max_to_normalize]  # uniq + cut
        resolved = 0
        prog = st.progress(0)
        status = st.empty()
        for i, h in enumerate(to_norm, start=1):
            status.write(f"üîÅ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è handle ‚Üí channel_id: @{h} ({i}/{len(to_norm)})")
            uc = resolve_handle_to_channel_id(youtube, h)
            if uc:
                if uc not in existing_ids:
                    existing_ids.add(uc)
                resolved += 1
            prog.progress(int(i / max(1, len(to_norm)) * 100))
        prog.empty()
        status.empty()

        stats["normalized"] = resolved
        stats["unresolved"] = max(0, len(to_norm) - resolved)

    return existing_ids, stats

# =============== SIDEBAR ===============
with st.sidebar:
    st.header("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞")
    max_pages_per_keyword = st.number_input("–ú–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∫–ª—é—á (–ø–æ 50 –≤–∏–¥–µ–æ)",
        min_value=1, max_value=10, value=3, step=1)
    max_channels_per_keyword = st.number_input("–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ –∫–ª—é—á",
        min_value=10, max_value=5000, value=500, step=10)
    max_recent_uploads_fetch = st.number_input("–ú–∞–∫—Å. –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –Ω–∞ –∫–∞–Ω–∞–ª (–¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–∞ –ø–µ—Ä–∏–æ–¥)",
        min_value=20, max_value=500, value=150, step=10)

    st.markdown("---")
    st.header("üìä –§–∏–ª—å—Ç—Ä—ã")
    min_subs = st.number_input("–ú–∏–Ω. –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤", value=1_000, step=500)
    max_subs = st.number_input("–ú–∞–∫—Å. –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤", value=500_000, step=1_000)
    min_views_total = st.number_input("–ú–∏–Ω. –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∫–∞–Ω–∞–ª–∞ (total)", value=10_000, step=1_000)
    period_days = st.number_input("–ü–µ—Ä–∏–æ–¥ (–¥–Ω–µ–π) –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ –≤–∏–¥–µ–æ", value=30, min_value=1, max_value=90)
    min_avg_views_period = st.number_input("–ú–∏–Ω. —Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥", value=2_000, step=100)

    st.markdown("---")
    st.header("üóÇÔ∏è –ò—Å–∫–ª—é—á–∏—Ç—å —É–∂–µ –∏–º–µ—é—â–∏—Ö—Å—è –±–ª–æ–≥–µ—Ä–æ–≤")
    uploaded_file = st.file_uploader(
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV (—Å–æ —Å—Ç–æ–ª–±—Ü–æ–º 'channel_id' –∏–ª–∏ —Å—Å—ã–ª–∫–∞–º–∏)", type=["xlsx", "csv"])

    normalize_handles = st.checkbox("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å @handle / /c/ /user/ –≤ channel_id (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)", True)
    treat_bare_names = st.checkbox("–°—á–∏—Ç–∞—Ç—å –≥–æ–ª—ã–µ –∏–º–µ–Ω–∞ –∫–∞–∫ handle (–º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å –ª–æ–∂–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)", False)
    max_to_normalize = st.number_input("–õ–∏–º–∏—Ç –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (—à—Ç.)", min_value=50, max_value=5000, value=800, step=50)

# =============== INPUT ===============
keywords_input = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ)")
keywords = [kw.strip() for kw in keywords_input.splitlines() if kw.strip()]

st.caption("‚ÑπÔ∏è –¢–µ–ø–µ—Ä—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏—Å–∫–ª—é—á–∞—é—Ç—Å—è –ø–æ –Ω–∞—Å—Ç–æ—è—â–µ–º—É channel_id. "
           "–ï—Å–ª–∏ –≤ –±–∞–∑–µ –±—ã–ª–∏ @handle –∏–ª–∏ custom URL, –≤–∫–ª—é—á–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –≤ —Å–∞–π–¥–±–∞—Ä–µ.")

# =============== SEARCH/ANALYTICS ===============
def search_channels_by_keyword(youtube, keyword: str, max_pages: int, max_channels: int) -> Dict[str, str]:
    channel_map: Dict[str, str] = {}
    page_token: Optional[str] = None
    pages = 0
    while True:
        try:
            resp = youtube.search().list(
                q=keyword, part="snippet", type="video", maxResults=50, pageToken=page_token
            ).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á—É ¬´{keyword}¬ª: {e}")
            break
        for it in resp.get("items", []):
            sn = it.get("snippet") or {}
            ch_id = sn.get("channelId")
            ch_title = sn.get("channelTitle")
            if ch_id and ch_id not in channel_map:
                channel_map[ch_id] = ch_title or ch_id
                if len(channel_map) >= max_channels:
                    return channel_map
        page_token = resp.get("nextPageToken")
        pages += 1
        if not page_token or pages >= max_pages:
            break
    return channel_map

def fetch_channels_stats(youtube, channel_ids: List[str]) -> Dict[str, Tuple[int, int, str, str]]:
    out: Dict[str, Tuple[int, int, str, str]] = {}
    for batch in chunked(channel_ids, 50):
        try:
            res = youtube.channels().list(
                part="statistics,snippet,contentDetails",
                id=",".join(batch)
            ).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤: {e}")
            continue
        for item in res.get("items", []):
            ch_id = item["id"]
            stats = item.get("statistics", {})
            snip = item.get("snippet", {})
            cdet = item.get("contentDetails", {})
            subs = int(stats.get("subscriberCount", 0) or 0)
            total_views = int(stats.get("viewCount", 0) or 0)
            country = snip.get("country", "N/A")
            uploads_pid = cdet.get("relatedPlaylists", {}).get("uploads", "")
            if uploads_pid:
                out[ch_id] = (subs, total_views, country, uploads_pid)
    return out

def iter_recent_upload_video_ids(youtube, uploads_playlist_id: str, since_dt: datetime, max_fetch: int) -> Iterable[str]:
    fetched = 0
    page_token: Optional[str] = None
    while True:
        try:
            pl = youtube.playlistItems().list(
                part="contentDetails",
                playlistId=uploads_playlist_id,
                maxResults=50,
                pageToken=page_token
            ).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–ª–µ–π–ª–∏—Å—Ç–∞ –∑–∞–≥—Ä—É–∑–æ–∫: {e}")
            return
        for it in pl.get("items", []):
            fetched += 1
            pub = iso_to_dt(it["contentDetails"]["videoPublishedAt"])
            if pub >= since_dt:
                yield it["contentDetails"]["videoId"]
            else:
                return
            if fetched >= max_fetch:
                return
        page_token = pl.get("nextPageToken")
        if not page_token:
            return

def get_avg_views_for_period(youtube, uploads_playlist_id: str, days: int, max_fetch: int) -> Tuple[Optional[int], int]:
    since_dt = datetime.now(timezone.utc) - timedelta(days=days)
    vids = list(iter_recent_upload_video_ids(youtube, uploads_playlist_id, since_dt, max_fetch=max_fetch))
    if not vids:
        return None, 0
    total_views = 0
    counted = 0
    for batch in chunked(vids, 50):
        try:
            vres = youtube.videos().list(part="statistics", id=",".join(batch)).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤–∏–¥–µ–æ: {e}")
            continue
        for it in vres.get("items", []):
            vc = int(it.get("statistics", {}).get("viewCount", 0) or 0)
            total_views += vc
            counted += 1
    if counted == 0:
        return None, 0
    return total_views // counted, counted

# =============== MAIN BUTTON ===============
if st.button("üîç –ù–∞–π—Ç–∏ –±–ª–æ–≥–µ—Ä–æ–≤"):
    if not API_KEY:
        st.error("‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω! –í—Å—Ç–∞–≤—å YOUTUBE_API_KEY –≤ Settings ‚Üí Secrets (Streamlit Cloud).")
        st.stop()
    if not keywords:
        st.error("‚ùå –í–≤–µ–¥–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ!")
        st.stop()

    youtube = build_client()

    # === 0) –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è ===
    existing_ids: set = set()
    stats_norm = {"taken_channel_id": 0, "extracted_from_url": 0, "normalized": 0, "unresolved": 0}
    if uploaded_file is not None:
        try:
            if uploaded_file.name.endswith(".csv"):
                df_existing = pd.read_csv(uploaded_file)
            else:
                df_existing = pd.read_excel(uploaded_file)

            existing_ids, stats_norm = normalize_existing_ids_from_df(
                youtube,
                df_existing,
                treat_bare_names=treat_bare_names,
                do_normalize=normalize_handles,
                max_to_normalize=int(max_to_normalize),
            )
            st.info(
                f"üìÇ –ë–∞–∑–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è: {len(existing_ids)} channel_id\n"
                f"‚Ä¢ —è–≤–Ω—ã–µ channel_id: {stats_norm['taken_channel_id']}\n"
                f"‚Ä¢ –∏–∑ —Å—Å—ã–ª–æ–∫ /channel/UC‚Ä¶: {stats_norm['extracted_from_url']}\n"
                f"‚Ä¢ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∏–∑ handle/custom: {stats_norm['normalized']}\n"
                f"‚Ä¢ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {stats_norm['unresolved']}"
            )
        except Exception as e:
            st.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –±–∞–∑—É: {e}")

    # === 1) –ü–æ–∏—Å–∫ –∫–∞–Ω–∞–ª–æ–≤ –ø–æ –∫–ª—é—á–∞–º (—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π) ===
    all_channels: Dict[str, str] = {}
    progress = st.progress(0)
    status = st.empty()

    for idx, kw in enumerate(keywords, start=1):
        status.write(f"üîé –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á—É ¬´{kw}¬ª ({idx}/{len(keywords)}) ‚Ä¶")
        found = search_channels_by_keyword(
            youtube, kw,
            max_pages=int(max_pages_per_keyword),
            max_channels=int(max_channels_per_keyword),
        )
        for ch_id, title in found.items():
            all_channels.setdefault(ch_id, title)
        progress.progress(int(idx / max(1, len(keywords)) * 20))  # –¥–æ 20%

    if not all_channels:
        st.warning("üòï –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –∫–ª—é—á–∞–º. –ü–æ–ø—Ä–æ–±—É–π –¥—Ä—É–≥–∏–µ –∫–ª—é—á–∏ –∏–ª–∏ —É–≤–µ–ª–∏—á—å –ø–∞–≥–∏–Ω–∞—Ü–∏—é.")
        st.stop()

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∏—Å–∫–ª—é—á–∏–º —É–∂–µ –∏–º–µ—é—â–∏–µ—Å—è –∫–∞–Ω–∞–ª—ã (—ç–∫–æ–Ω–æ–º–∏–º –∫–≤–æ—Ç—É)
    before = len(all_channels)
    all_channels = {cid: t for cid, t in all_channels.items() if cid not in existing_ids}
    pre_excluded = before - len(all_channels)

    if pre_excluded:
        st.caption(f"üßπ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∏—Å–∫–ª—é—á–µ–Ω–æ –ø–æ –±–∞–∑–µ: {pre_excluded} –∫–∞–Ω–∞–ª(–æ–≤).")

    # === 2) –¢—è–Ω–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤ –±–∞—Ç—á–∞–º–∏ ===
    status.write(f"üì¶ –ü–æ–ª—É—á–∞—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤ (–≤—Å–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_channels)}) ‚Ä¶")
    stats_map = fetch_channels_stats(youtube, list(all_channels.keys()))
    progress.progress(60)

    # === 3) –ë–∞–∑–æ–≤—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã ===
    base_pass_ids = []
    for ch_id, tpl in stats_map.items():
        subs, total_views, _country, _uploads = tpl
        if min_subs <= subs <= max_subs and total_views >= min_views_total:
            base_pass_ids.append(ch_id)

    if not base_pass_ids:
        st.warning("üòï –ù–∏ –æ–¥–∏–Ω –∫–∞–Ω–∞–ª –Ω–µ –ø—Ä–æ—à—ë–ª —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º/total –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º.")
        st.stop()

    # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ –±–∞–∑–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
    base_pass_ids = [cid for cid in base_pass_ids if cid not in existing_ids]

    # === 4) –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥ ===
    results: List[Dict] = []
    for i, ch_id in enumerate(base_pass_ids, start=1):
        title = all_channels.get(ch_id, ch_id)
        subs, total_views, country, uploads_pid = stats_map[ch_id]
        status.write(f"‚è±Ô∏è {i}/{len(base_pass_ids)} ‚Ä¢ –°—á–∏—Ç–∞—é —Å—Ä–µ–¥–Ω–∏–µ –∑–∞ {int(period_days)} –¥–Ω.: {title}")
        avg_views, n_vids = get_avg_views_for_period(
            youtube, uploads_pid, int(period_days), max_fetch=int(max_recent_uploads_fetch)
        )
        if avg_views is None or avg_views < int(min_avg_views_period):
            progress.progress(60 + int(40 * i / max(1, len(base_pass_ids))))
            continue

        results.append({
            "–ö–∞–Ω–∞–ª": title,
            "–ü–æ–¥–ø–∏—Å—á–∏–∫–∏": subs,
            "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã (total)": total_views,
            f"–°—Ä–µ–¥–Ω–∏–µ –∑–∞ {int(period_days)} –¥–Ω.": avg_views,
            f"–í–∏–¥–µ–æ –∑–∞ {int(period_days)} –¥–Ω.": n_vids,
            "–°—Ç—Ä–∞–Ω–∞": country,
            "–°—Å—ã–ª–∫–∞": f"https://www.youtube.com/channel/{ch_id}",
            "channel_id": ch_id,
        })
        progress.progress(60 + int(40 * i / max(1, len(base_pass_ids))))

    status.empty()
    progress.empty()

    # === 5) –í—ã–≤–æ–¥ ===
    if results:
        df = pd.DataFrame(results)
        df.drop_duplicates(subset=["channel_id"], inplace=True)
        sort_col = f"–°—Ä–µ–¥–Ω–∏–µ –∑–∞ {int(period_days)} –¥–Ω."
        df.sort_values(by=[sort_col, "–ü–æ–¥–ø–∏—Å—á–∏–∫–∏"], ascending=[False, False], inplace=True)

        st.success(
            f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(df)} –Ω–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ "
            f"(–ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è {len(existing_ids)} –∏–∑ –±–∞–∑—ã –∏ –ø—Ä–µ–¥—Ñ–∏–ª—å—Ç—Ä–∞ {pre_excluded})."
        )
        st.dataframe(df.drop(columns=["channel_id"]), use_container_width=True)

        excel_file = "bloggers.xlsx"
        df.to_excel(excel_file, index=False)
        with open(excel_file, "rb") as f:
            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å Excel",
                data=f,
                file_name="bloggers.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
    else:
        st.warning("üòï –ü–æ –∑–∞–¥–∞–Ω–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (–ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤/—Ñ–∏–ª—å—Ç—Ä–æ–≤). "
                   "–û—Å–ª–∞–±—å —Ñ–∏–ª—å—Ç—Ä—ã, —É–≤–µ–ª–∏—á—å –ø–µ—Ä–∏–æ–¥ –∏–ª–∏ –≤—ã–∫–ª—é—á–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è —Ç–µ—Å—Ç–∞.")
