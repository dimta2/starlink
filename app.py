# StarLinker v. 1.3
# a tool for searching YouTube influencers
# (c) StarPets 2025

import os
import re
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Tuple, Optional, Iterable

import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from dotenv import load_dotenv

# =================== –ù–ê–°–¢–†–û–ô–ö–ê –°–¢–†–ê–ù–ò–¶–´ ===================
st.set_page_config(page_title="StarLinker", page_icon="üîé", layout="wide")

# =================== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===================
load_dotenv()
# —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º Streamlit secrets, –ø–æ—Ç–æ–º .env
api_key = os.getenv("YOUTUBE_API_KEY")
try:
    import streamlit as st
    if not api_key:
        api_key = st.secrets.get("YOUTUBE_API_KEY")
except Exception:
    pass

API_KEY = os.getenv("YOUTUBE_API_KEY")

st.title("üîé StarLinker ‚Äî –ü–æ–∏—Å–∫ –±–ª–æ–≥–µ—Ä–æ–≤ –Ω–∞ YouTube (v1.3)")

# =================== –•–ï–õ–ü–ï–†–´ ===================
def chunked(iterable: List[str], size: int) -> Iterable[List[str]]:
    for i in range(0, len(iterable), size):
        yield iterable[i:i + size]

def iso_to_dt(s: str) -> datetime:
    return datetime.fromisoformat(s.replace("Z", "+00:00"))

def build_client(api_key: str):
    return build("youtube", "v3", developerKey=api_key)

def extract_channel_id(url_or_id: str) -> str:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –≤—ã—Ç–∞—â–∏—Ç—å channel_id (–æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 'UC').
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
      - https://www.youtube.com/channel/UCxxxx
      - –ø—Ä–æ—Å—Ç–æ 'UCxxxx' (—á–∏—Å—Ç—ã–π id –≤ —è—á–µ–π–∫–µ)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç '' –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å.
    """
    if not isinstance(url_or_id, str):
        return ""
    s = url_or_id.strip()
    # —á–∏—Å—Ç—ã–π UC-id
    if re.fullmatch(r"UC[0-9A-Za-z_-]{20,}", s):
        return s
    # —Å—Å—ã–ª–∫–∞ —Å /channel/UC...
    m = re.search(r"(?:/channel/)(UC[0-9A-Za-z_-]{20,})", s)
    if m:
        return m.group(1)
    return ""

# =================== –°–ê–ô–î–ë–ê–†: –ü–ê–†–ê–ú–ï–¢–†–´ ===================
with st.sidebar:
    st.header("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞")
    max_pages_per_keyword = st.number_input(
        "–ú–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∫–ª—é—á (–ø–æ 50 –≤–∏–¥–µ–æ)",
        min_value=1, max_value=10, value=3, step=1,
        help="–ü–∞–≥–∏–Ω–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ YouTube. 1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞ = –¥–æ 50 –≤–∏–¥–µ–æ."
    )
    max_channels_per_keyword = st.number_input(
        "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ –∫–ª—é—á",
        min_value=10, max_value=5000, value=500, step=10,
        help="–ß—Ç–æ–±—ã –Ω–µ —Å–∂–∏–≥–∞—Ç—å –∫–≤–æ—Ç—É, –º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å —á–∏—Å–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ –∫–ª—é—á."
    )
    max_recent_uploads_fetch = st.number_input(
        "–ú–∞–∫—Å. –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –Ω–∞ –∫–∞–Ω–∞–ª (–¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–∞ –ø–µ—Ä–∏–æ–¥)",
        min_value=20, max_value=500, value=150, step=10,
        help="–°–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –≤–∏–¥–µ–æ —Å–º–æ—Ç—Ä–µ—Ç—å –∏–∑ –ø–ª–µ–π–ª–∏—Å—Ç–∞ –∑–∞–≥—Ä—É–∑–æ–∫ –∫–∞–Ω–∞–ª–∞."
    )

    st.markdown("---")
    st.header("üìä –§–∏–ª—å—Ç—Ä—ã")
    min_subs = st.number_input("–ú–∏–Ω. –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤", value=1_000, step=500)
    max_subs = st.number_input("–ú–∞–∫—Å. –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤", value=500_000, step=1_000)
    min_views_total = st.number_input("–ú–∏–Ω. –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∫–∞–Ω–∞–ª–∞ (total)", value=10_000, step=1_000)

    period_days = st.number_input("–ü–µ—Ä–∏–æ–¥ (–¥–Ω–µ–π) –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ –≤–∏–¥–µ–æ", value=30, min_value=1, max_value=90)
    min_avg_views_period = st.number_input("–ú–∏–Ω. —Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥", value=2_000, step=100)

    st.markdown("---")
    st.header("üóÇÔ∏è –ò—Å–∫–ª—é—á–∏—Ç—å —É–∂–µ –∏–º–µ—é—â–∏—Ö—Å—è –±–ª–æ–≥–µ—Ä–æ–≤")
    uploaded_file = st.file_uploader(
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel/CSV –∏–∑ –≤–∞—à–µ–π –±–∞–∑—ã (—Å–æ —Å—Ç–æ–ª–±—Ü–æ–º '–°—Å—ã–ª–∫–∞' –∏–ª–∏ 'channel_id')",
        type=["xlsx", "csv"],
        help="–≠—Ç–∏ –∫–∞–Ω–∞–ª—ã –±—É–¥—É—Ç –∏—Å–∫–ª—é—á–µ–Ω—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–∏—Å–∫–∞."
    )

# =================== –ó–ê–ì–†–£–ó–ö–ê –ë–ê–ó–´ –î–õ–Ø –ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø ===================
existing_ids: set[str] = set()
if uploaded_file is not None:
    try:
        if uploaded_file.name.endswith(".csv"):
            df_existing = pd.read_csv(uploaded_file)
        else:
            df_existing = pd.read_excel(uploaded_file)

        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        cols_lower = {c.lower(): c for c in df_existing.columns}
        channel_id_col = None
        link_col = None

        # –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —è–≤–Ω—ã–π channel_id
        for k, orig in cols_lower.items():
            if "channel_id" == k or k.endswith("channel_id") or k == "id":
                channel_id_col = orig
                break
        if not channel_id_col:
            # –∏—â–µ–º —Å—Å—ã–ª–∫—É
            for k, orig in cols_lower.items():
                if "—Å—Å—ã–ª" in k or "link" in k or "url" in k:
                    link_col = orig
                    break

        if channel_id_col:
            for v in df_existing[channel_id_col].dropna():
                ch = extract_channel_id(str(v))
                if ch:
                    existing_ids.add(ch)
        elif link_col:
            for v in df_existing[link_col].dropna():
                ch = extract_channel_id(str(v))
                if ch:
                    existing_ids.add(ch)
        else:
            st.warning("‚ö†Ô∏è –ù–µ –Ω–∞—à–ª–∏ —Å—Ç–æ–ª–±—Ü—ã 'channel_id' –∏–ª–∏ '–°—Å—ã–ª–∫–∞/Link' ‚Äî –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –ø—Ä–∏–º–µ–Ω–∏—Ç—Å—è.")

        st.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(existing_ids)} –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã: {e}")

# =================== –í–í–û–î –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í ===================
keywords_input = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ)")
keywords = [kw.strip() for kw in keywords_input.splitlines() if kw.strip()]

st.caption("‚ÑπÔ∏è –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: —É–≤–µ–ª–∏—á—å —á–∏—Å–ª–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –ª–∏–º–∏—Ç –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ –∫–ª—é—á, –µ—Å–ª–∏ –Ω–∞—Ö–æ–¥–æ–∫ –º–∞–ª–æ. "
           "–°–ª–∏—à–∫–æ–º —Å—Ç—Ä–æ–≥–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã —Ç–æ–∂–µ –º–æ–≥—É—Ç –æ–±–Ω—É–ª—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")

# =================== API-–û–ë–û–õ–û–ß–ö–ò ===================
def search_channels_by_keyword(youtube, keyword: str, max_pages: int, max_channels: int) -> Dict[str, str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {channel_id: channel_title} –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∫–ª—é—á–∞, –ø—Ä–æ—Ö–æ–¥—è –¥–æ max_pages —Å—Ç—Ä–∞–Ω–∏—Ü."""
    channel_map: Dict[str, str] = {}
    page_token: Optional[str] = None
    pages = 0

    while True:
        try:
            resp = youtube.search().list(
                q=keyword,
                part="snippet",
                type="video",
                maxResults=50,
                pageToken=page_token
            ).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á—É ¬´{keyword}¬ª: {e}")
            break

        for it in resp.get("items", []):
            ch_id = it["snippet"]["channelId"]
            ch_title = it["snippet"]["channelTitle"]
            if ch_id not in channel_map:
                channel_map[ch_id] = ch_title
                if len(channel_map) >= max_channels:
                    return channel_map

        page_token = resp.get("nextPageToken")
        pages += 1
        if not page_token or pages >= max_pages:
            break

    return channel_map

def fetch_channels_stats(youtube, channel_ids: List[str]) -> Dict[str, Tuple[int, int, str, str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å:
      channel_id -> (subs, total_views, country, uploads_playlist_id)
    """
    out: Dict[str, Tuple[int, int, str, str]] = {}
    for batch in chunked(channel_ids, 50):
        try:
            res = youtube.channels().list(
                part="statistics,snippet,contentDetails",
                id=",".join(batch)
            ).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤: {e}")
            continue

        for item in res.get("items", []):
            ch_id = item["id"]
            stats = item.get("statistics", {})
            snip = item.get("snippet", {})
            cdet = item.get("contentDetails", {})
            subs = int(stats.get("subscriberCount", 0) or 0)
            total_views = int(stats.get("viewCount", 0) or 0)
            country = snip.get("country", "N/A")
            uploads_pid = cdet.get("relatedPlaylists", {}).get("uploads", "")
            if uploads_pid:
                out[ch_id] = (subs, total_views, country, uploads_pid)
    return out

def iter_recent_upload_video_ids(
    youtube, uploads_playlist_id: str, since_dt: datetime, max_fetch: int
) -> Iterable[str]:
    fetched = 0
    page_token: Optional[str] = None
    while True:
        try:
            pl = youtube.playlistItems().list(
                part="contentDetails",
                playlistId=uploads_playlist_id,
                maxResults=50,
                pageToken=page_token
            ).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–ª–µ–π–ª–∏—Å—Ç–∞ –∑–∞–≥—Ä—É–∑–æ–∫: {e}")
            return

        for it in pl.get("items", []):
            fetched += 1
            pub = iso_to_dt(it["contentDetails"]["videoPublishedAt"])
            if pub >= since_dt:
                yield it["contentDetails"]["videoId"]
            else:
                # –ü–æ—Ä—è–¥–æ–∫: –Ω–æ–≤—ã–µ ‚Üí —Å—Ç–∞—Ä—ã–µ. –°—Ç–∞—Ä—à–µ –ø–µ—Ä–∏–æ–¥–∞ ‚Äî –¥–∞–ª—å—à–µ –º–æ–∂–Ω–æ –Ω–µ –∏–¥—Ç–∏.
                return
            if fetched >= max_fetch:
                return

        page_token = pl.get("nextPageToken")
        if not page_token:
            return

def get_avg_views_for_period(
    youtube, uploads_playlist_id: str, days: int, max_fetch: int
) -> Tuple[Optional[int], int]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (avg_views_or_None, count_videos_in_period).
    None ‚Äî –µ—Å–ª–∏ –∑–∞ –ø–µ—Ä–∏–æ–¥ –Ω–µ—Ç –≤–∏–¥–µ–æ.
    """
    since_dt = datetime.now(timezone.utc) - timedelta(days=days)
    vids = list(iter_recent_upload_video_ids(youtube, uploads_playlist_id, since_dt, max_fetch=max_fetch))
    if not vids:
        return None, 0

    total_views = 0
    counted = 0
    for batch in chunked(vids, 50):
        try:
            vres = youtube.videos().list(part="statistics", id=",".join(batch)).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤–∏–¥–µ–æ: {e}")
            continue
        for it in vres.get("items", []):
            vc = int(it.get("statistics", {}).get("viewCount", 0) or 0)
            total_views += vc
            counted += 1

    if counted == 0:
        return None, 0
    return total_views // counted, counted

# =================== –û–°–ù–û–í–ù–û–ô –ë–õ–û–ö ===================
if st.button("üîç –ù–∞–π—Ç–∏ –±–ª–æ–≥–µ—Ä–æ–≤"):
    if not API_KEY:
        st.error("‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å —Ñ–∞–π–ª .env (YOUTUBE_API_KEY=...)")
    elif not keywords:
        st.error("‚ùå –í–≤–µ–¥–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ!")
    else:
        youtube = build_client(API_KEY)

        all_channels: Dict[str, str] = {}
        progress = st.progress(0)
        status = st.empty()

        # 1) –ü–æ–∏—Å–∫ –∫–∞–Ω–∞–ª–æ–≤ –ø–æ –∫–ª—é—á–∞–º (—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π)
        for idx, kw in enumerate(keywords, start=1):
            status.write(f"üîé –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á—É ¬´{kw}¬ª ({idx}/{len(keywords)}) ‚Ä¶")
            found = search_channels_by_keyword(
                youtube, kw,
                max_pages=int(max_pages_per_keyword),
                max_channels=int(max_channels_per_keyword),
            )
            # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ
            for ch_id, title in found.items():
                all_channels.setdefault(ch_id, title)

            progress.progress(int(idx / max(1, len(keywords)) * 33))  # –¥–æ ~33%

        if not all_channels:
            st.warning("üòï –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º. –ü–æ–ø—Ä–æ–±—É–π –¥—Ä—É–≥–∏–µ –∫–ª—é—á–∏ –∏–ª–∏ —É–≤–µ–ª–∏—á—å –ø–∞–≥–∏–Ω–∞—Ü–∏—é.")
            st.stop()

        # 2) –¢—è–Ω–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤ –±–∞—Ç—á–∞–º–∏
        status.write(f"üì¶ –ü–æ–ª—É—á–∞—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤ (–≤—Å–µ–≥–æ {len(all_channels)}) ‚Ä¶")
        stats_map = fetch_channels_stats(youtube, list(all_channels.keys()))
        progress.progress(60)

        # 3) –ë–∞–∑–æ–≤—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –∫–∞–Ω–∞–ª—É
        base_pass_ids = []
        for ch_id, tpl in stats_map.items():
            subs, total_views, _country, _uploads = tpl
            if min_subs <= subs <= max_subs and total_views >= min_views_total:
                base_pass_ids.append(ch_id)

        if not base_pass_ids:
            st.warning("üòï –ù–∏ –æ–¥–∏–Ω –∫–∞–Ω–∞–ª –Ω–µ –ø—Ä–æ—à—ë–ª —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º/total –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º.")
            st.stop()

        # 4) –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥ + —Ñ–∏–ª—å—Ç—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –±–∞–∑—ã
        results: List[Dict] = []
        skipped_existing = 0

        for i, ch_id in enumerate(base_pass_ids, start=1):
            title = all_channels.get(ch_id, ch_id)

            # –∏—Å–∫–ª—é—á–∞–µ–º, –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –≤ —Ç–≤–æ–µ–π –±–∞–∑–µ
            if ch_id in existing_ids:
                skipped_existing += 1
                # –æ–±–Ω–æ–≤–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å –æ—Ç 60% –∫ 100% —Å —É—á—ë—Ç–æ–º –ø—Ä–æ–ø—É—Å–∫–∞
                progress.progress(60 + int(40 * i / max(1, len(base_pass_ids))))
                continue

            subs, total_views, country, uploads_pid = stats_map[ch_id]
            status.write(f"‚è±Ô∏è {i}/{len(base_pass_ids)} ‚Ä¢ –°—á–∏—Ç–∞—é —Å—Ä–µ–¥–Ω–∏–µ –∑–∞ {int(period_days)} –¥–Ω. –¥–ª—è: {title}")
            avg_views, n_vids = get_avg_views_for_period(
                youtube, uploads_pid, int(period_days), max_fetch=int(max_recent_uploads_fetch)
            )

            if avg_views is None or avg_views < int(min_avg_views_period):
                progress.progress(60 + int(40 * i / max(1, len(base_pass_ids))))
                continue

            results.append({
                "–ö–∞–Ω–∞–ª": title,
                "–ü–æ–¥–ø–∏—Å—á–∏–∫–∏": subs,
                "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã (total)": total_views,
                f"–°—Ä–µ–¥–Ω–∏–µ –∑–∞ {int(period_days)} –¥–Ω.": avg_views,
                f"–í–∏–¥–µ–æ –∑–∞ {int(period_days)} –¥–Ω.": n_vids,
                "–°—Ç—Ä–∞–Ω–∞": country,
                "–°—Å—ã–ª–∫–∞": f"https://www.youtube.com/channel/{ch_id}",
                "channel_id": ch_id,  # —É–¥–æ–±–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π
            })

            progress.progress(60 + int(40 * i / max(1, len(base_pass_ids))))

        status.empty()
        progress.empty()

        # 5) –í—ã–≤–æ–¥ –∏ –≤—ã–≥—Ä—É–∑–∫–∞
        if results:
            df = pd.DataFrame(results)
            df.drop_duplicates(subset=["channel_id"], inplace=True)
            sort_col = f"–°—Ä–µ–¥–Ω–∏–µ –∑–∞ {int(period_days)} –¥–Ω."
            df.sort_values(by=[sort_col, "–ü–æ–¥–ø–∏—Å—á–∏–∫–∏"], ascending=[False, False], inplace=True)

            msg = f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(df)} –Ω–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤"
            if uploaded_file is not None:
                msg += f" (–∏—Å–∫–ª—é—á–µ–Ω–æ –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç—ã: {skipped_existing})"
            st.success(msg)

            st.dataframe(df.drop(columns=["channel_id"]), use_container_width=True)

            excel_file = "bloggers.xlsx"
            df.to_excel(excel_file, index=False)
            with open(excel_file, "rb") as f:
                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å Excel",
                    data=f,
                    file_name="bloggers.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        else:
            if uploaded_file is not None and skipped_existing > 0:
                st.warning("–í—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –æ–∫–∞–∑–∞–ª–∏—Å—å –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –±–∞–∑–µ. –ü–æ–ø—Ä–æ–±—É–π –¥—Ä—É–≥–∏–µ –∫–ª—é—á–∏ –∏–ª–∏ –æ—Å–ª–∞–±—å —Ñ–∏–ª—å—Ç—Ä—ã.")
            else:
                st.warning("üòï –ü–æ –∑–∞–¥–∞–Ω–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –û—Å–ª–∞–±—å —Ñ–∏–ª—å—Ç—Ä—ã –∏–ª–∏ —É–≤–µ–ª–∏—á—å –ø–µ—Ä–∏–æ–¥/—Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞.")
