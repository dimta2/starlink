import os
os.environ["STREAMLIT_SERVER_FILEWATCHER_TYPE"] = "none"

import streamlit as st
import pandas as pd

from starlinker.config import get_api_key
from starlinker.youtube_client import get_youtube_client
from starlinker.search import search_channels_by_keyword
from starlinker.channel_stats import fetch_channels_stats, get_avg_views_for_period
from starlinker.dedupe import load_existing_ids
from starlinker.quota import init_budget, set_budget, used, budget

st.set_page_config(page_title="StarLinker", page_icon="üîé", layout="wide")
st.title("üîé StarLinker ‚Äî –ü–æ–∏—Å–∫ –±–ª–æ–≥–µ—Ä–æ–≤ –Ω–∞ YouTube")

API_KEY = get_api_key()
if not API_KEY:
    st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω YOUTUBE_API_KEY. –í Cloud –¥–æ–±–∞–≤—å –≤ Settings ‚Üí Secrets.")
    st.stop()

init_budget()

with st.sidebar:
    st.header("‚öôÔ∏è –ü–æ–∏—Å–∫")
    max_pages_per_keyword = st.number_input("–°—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∫–ª—é—á (x50 –≤–∏–¥–µ–æ)", 1, 5, 2)  # –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    max_channels_per_keyword = st.number_input("–õ–∏–º–∏—Ç –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ –∫–ª—é—á", 10, 1000, 200)
    max_recent_uploads_fetch = st.number_input("–í–∏–¥–µ–æ –Ω–∞ –∫–∞–Ω–∞–ª –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ", 20, 200, 80, 10)
    direct_channel_search = st.checkbox("–ò—Å–∫–∞—Ç—å –ø–æ –∫–∞–Ω–∞–ª–∞–º –Ω–∞–ø—Ä—è–º—É—é (type=channel)", False)

    st.markdown("---")
    st.header("üìä –§–∏–ª—å—Ç—Ä—ã")
    min_subs = st.number_input("–ú–∏–Ω. –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤", value=1_000, step=500)
    max_subs = st.number_input("–ú–∞–∫—Å. –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤", value=500_000, step=1_000)
    min_views_total = st.number_input("–ú–∏–Ω. –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ (total)", value=10_000, step=1_000)
    period_days = st.number_input("–ü–µ—Ä–∏–æ–¥ –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ (–¥–Ω–µ–π)", 1, 90, 30)
    min_avg_views_period = st.number_input("–ú–∏–Ω. —Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥", value=2_000, step=100)

    st.markdown("---")
    st.header("üóÇÔ∏è –ë–∞–∑–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è")
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏ Excel/CSV (channel_id/–°—Å—ã–ª–∫–∞/Link/URL/handle)", type=["xlsx", "csv"])
    normalize_handles = st.checkbox("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å @handle ‚Üí channel_id (–¥–æ—Ä–æ–≥–æ!)", False)  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –í–´–ö–õ
    treat_bare_names = st.checkbox("–°—á–∏—Ç–∞—Ç—å –≥–æ–ª—ã–µ –∏–º–µ–Ω–∞ –∫–∞–∫ handle", False)
    max_to_normalize = st.number_input("–õ–∏–º–∏—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏", 0, 500, 50, 10)

    st.markdown("---")
    st.header("‚õΩ –ö–≤–æ—Ç–∞ YouTube (—é–Ω–∏—Ç—ã)")
    b = st.number_input("–ë—é–¥–∂–µ—Ç –Ω–∞ –∑–∞–ø—É—Å–∫", 500, 10000, 3000, 100)
    set_budget(b)
    st.caption("search.list = 100 u; channels/videos/playlistItems = 1 u. "
               "–ü—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ ‚Äî –ø—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å —Å–æ–æ–±—â–µ–Ω–∏–µ–º.")

    st.markdown("---")
    debug_mode = st.checkbox("–†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏", True)

keywords_input = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ)")
keywords = [k.strip() for k in keywords_input.splitlines() if k.strip()]

if st.button("üîç –ù–∞–π—Ç–∏ –±–ª–æ–≥–µ—Ä–æ–≤"):
    try:
        if not keywords:
            st.error("‚ùå –í–≤–µ–¥–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ!")
            st.stop()

        yt = get_youtube_client(API_KEY)

        # 0) existing_ids / existing_handles
        existing_ids = set()
        existing_handles = set()
        stats_norm = {}
        if uploaded_file is not None:
            try:
                df_old = pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv") else pd.read_excel(uploaded_file)
                from starlinker.dedupe import load_existing_ids as _load
                existing_ids, existing_handles, stats_norm = _load(
                    yt, df_old,
                    treat_bare_names=treat_bare_names,
                    do_normalize=normalize_handles,
                    max_to_normalize=int(max_to_normalize)
                )
                if debug_mode:
                    st.info(
                        f"üìÇ –ë–∞–∑–∞: {len(existing_ids)} –ø–æ channel_id, {len(existing_handles)} –ø–æ handle. "
                        f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: {stats_norm.get('normalized',0)}."
                    )
            except Exception as e:
                st.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –±–∞–∑—É: {e}")

        # 1) –ø–æ–∏—Å–∫
        all_channels = {}
        prog = st.progress(0); stat = st.empty()
        for i, kw in enumerate(keywords, start=1):
            stat.write(f"üîé –ü–æ–∏—Å–∫ ¬´{kw}¬ª ({i}/{len(keywords)}) ‚Ä¶")
            found = search_channels_by_keyword(
                yt, kw,
                max_pages=max_pages_per_keyword,
                max_channels=max_channels_per_keyword,
                by_channel=direct_channel_search
            )
            if debug_mode:
                st.caption(f"‚Ä¢ –Ω–∞–π–¥–µ–Ω–æ –ø–æ ¬´{kw}¬ª: {len(found)} –∫–∞–Ω–∞–ª–æ–≤ (–∫–≤–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞: {used()}/{budget()})")
            for cid, title in found.items():
                all_channels.setdefault(cid, title)
            prog.progress(int(i / len(keywords) * 20))

        if not all_channels:
            st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —ç—Ç–∞–ø–µ –ø–æ–∏—Å–∫–∞.")
            st.stop()

        # 1.1) –∏—Å–∫–ª—é—á–∏—Ç—å –ø–æ id –¥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        before = len(all_channels)
        all_channels = {cid: t for cid, t in all_channels.items() if cid not in existing_ids}
        if debug_mode:
            st.caption(f"üßπ –ò—Å–∫–ª—é—á–µ–Ω–æ –ø–æ channel_id: {before - len(all_channels)}")

        # 2) —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–Ω–∞–ª–æ–≤
        stat.write(f"üì¶ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–Ω–∞–ª–æ–≤ ({len(all_channels)}) ‚Ä¶")
        stats_map = fetch_channels_stats(yt, list(all_channels.keys()))
        prog.progress(55)
        if debug_mode:
            st.caption(f"‚Ä¢ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª—É—á–µ–Ω–∞: {len(stats_map)} (–∫–≤–æ—Ç–∞: {used()}/{budget()})")

        # 2.1) –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –ø–æ handle (customUrl)
        def _norm(x: str | None) -> str:
            return (x or "").lstrip("@").lower()
        if existing_handles:
            drop_ids = [cid for cid, s in stats_map.items() if _norm(s.get("custom_url")) in existing_handles]
            for cid in drop_ids:
                stats_map.pop(cid, None); all_channels.pop(cid, None)
            if debug_mode and drop_ids:
                st.caption(f"üßπ –ò—Å–∫–ª—é—á–µ–Ω–æ –ø–æ handle: {len(drop_ids)}")

        # 3) —Ñ–∏–ª—å—Ç—Ä—ã
        base_pass = [
            cid for cid, s in stats_map.items()
            if s.get("uploads_playlist_id")
            and (min_subs <= s.get("subs", 0) <= max_subs)
            and (s.get("total_views", 0) >= min_views_total)
        ]
        if debug_mode:
            st.caption(f"‚Ä¢ –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä –ø–æ–¥–ø–∏—Å—á–∏–∫–∏/total: {len(base_pass)}")

        if not base_pass:
            st.warning("–ù–∏–∫—Ç–æ –Ω–µ –ø—Ä–æ—à—ë–ª —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º/–ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º.")
            st.stop()

        # 4) —Å—Ä–µ–¥–Ω–∏–µ –∑–∞ –ø–µ—Ä–∏–æ–¥
        rows = []
        for i, cid in enumerate(base_pass, start=1):
            title = all_channels.get(cid, cid)
            s = stats_map[cid]
            stat.write(f"‚è±Ô∏è {i}/{len(base_pass)} ‚Ä¢ –°—Ä–µ–¥–Ω–∏–µ –∑–∞ {period_days} –¥–Ω.: {title}")
            avg, count = get_avg_views_for_period(yt, s["uploads_playlist_id"], period_days, max_recent_uploads_fetch)
            if avg is None or avg < min_avg_views_period:
                prog.progress(60 + int(40 * i / len(base_pass)))
                continue
            rows.append({
                "–ö–∞–Ω–∞–ª": title,
                "–ü–æ–¥–ø–∏—Å—á–∏–∫–∏": s["subs"],
                "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã_total": s["total_views"],
                "–°—Ä–µ–¥–Ω–∏–µ_–∑–∞_–¥–Ω–∏": avg,
                "–í–∏–¥–µ–æ_–∑–∞_–¥–Ω–∏": count,
                "–°—Ç—Ä–∞–Ω–∞": s["country"],
                "–°—Å—ã–ª–∫–∞": f"https://www.youtube.com/channel/{cid}",
                "channel_id": cid
            })
            if debug_mode and i % 5 == 0:
                st.caption(f"‚Ä¢ –∫–≤–æ—Ç–∞: {used()}/{budget()}")
            prog.progress(60 + int(40 * i / len(base_pass)))

        stat.empty(); prog.empty()

        if rows:
            df = pd.DataFrame(rows).drop_duplicates(subset=["channel_id"])
            df = df.sort_values(by=["–°—Ä–µ–¥–Ω–∏–µ_–∑–∞_–¥–Ω–∏", "–ü–æ–¥–ø–∏—Å—á–∏–∫–∏"], ascending=[False, False])
            st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(df)} –Ω–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ ‚Ä¢ –ö–≤–æ—Ç–∞ –∏–∑—Ä–∞—Å—Ö–æ–¥–æ–≤–∞–Ω–∞: {used()}/{budget()} —é–Ω–∏—Ç–æ–≤")
            st.dataframe(df.drop(columns=["channel_id"]), use_container_width=True)
            xlsx = "bloggers.xlsx"
            df.to_excel(xlsx, index=False)
            with open(xlsx, "rb") as f:
                st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel", f, file_name="bloggers.xlsx")
        else:
            st.warning("–ü–æ—Å–ª–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ –Ω–∏—á–µ–≥–æ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å.")
    except RuntimeError as e:
        st.error(f"‚õΩ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∫–≤–æ—Ç—ã: {e}")
