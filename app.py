# Starlinker Rolling build B280925A

import streamlit as st
import pandas as pd
from starlinker.config import get_api_key
from starlinker.youtube_client import get_youtube_client
from starlinker.search import search_channels_by_keyword
from starlinker.channel_stats import fetch_channels_stats, get_avg_views_for_period
from starlinker.dedupe import load_existing_ids

st.set_page_config(page_title="StarLinker", page_icon="üîé", layout="wide")
st.title("üîé StarLinker ‚Äî –ü–æ–∏—Å–∫ –±–ª–æ–≥–µ—Ä–æ–≤ –Ω–∞ YouTube Rolling")

API_KEY = get_api_key()
if not API_KEY:
    st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω YOUTUBE_API_KEY. –í Cloud –¥–æ–±–∞–≤—å –≤ Settings ‚Üí Secrets.")
    st.stop()

with st.sidebar:
    st.header("‚öôÔ∏è –ü–æ–∏—Å–∫")
    max_pages_per_keyword = st.number_input("–°—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∫–ª—é—á (x50 –≤–∏–¥–µ–æ)", 1, 10, 3)
    max_channels_per_keyword = st.number_input("–õ–∏–º–∏—Ç –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ –∫–ª—é—á", 10, 5000, 500)
    max_recent_uploads_fetch = st.number_input("–í–∏–¥–µ–æ –Ω–∞ –∫–∞–Ω–∞–ª –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ", 20, 500, 150, 10)

    st.markdown("---")
    st.header("üìä –§–∏–ª—å—Ç—Ä—ã")
    min_subs = st.number_input("–ú–∏–Ω. –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤", value=1_000, step=500)
    max_subs = st.number_input("–ú–∞–∫—Å. –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤", value=500_000, step=1_000)
    min_views_total = st.number_input("–ú–∏–Ω. –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ (total)", value=10_000, step=1_000)
    period_days = st.number_input("–ü–µ—Ä–∏–æ–¥ –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ (–¥–Ω–µ–π)", 1, 90, 30)
    min_avg_views_period = st.number_input("–ú–∏–Ω. —Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥", value=2_000, step=100)

    st.markdown("---")
    st.header("üóÇÔ∏è –ë–∞–∑–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è")
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏ Excel/CSV (channel_id/–°—Å—ã–ª–∫–∞/Link/URL)", type=["xlsx", "csv"])
    normalize_handles = st.checkbox("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å @handle / /c/ /user/ ‚Üí channel_id", True)
    treat_bare_names = st.checkbox("–°—á–∏—Ç–∞—Ç—å –≥–æ–ª—ã–µ –∏–º–µ–Ω–∞ –∫–∞–∫ handle", False)
    max_to_normalize = st.number_input("–õ–∏–º–∏—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏", 50, 5000, 800, 50)

keywords_input = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ)")
keywords = [k.strip() for k in keywords_input.splitlines() if k.strip()]

if st.button("üîç –ù–∞–π—Ç–∏ –±–ª–æ–≥–µ—Ä–æ–≤"):
    if not keywords:
        st.error("‚ùå –í–≤–µ–¥–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ!")
        st.stop()

    yt = get_youtube_client(API_KEY)

    # 0) existing_ids
    existing_ids = set(); stats_norm = {}
    if uploaded_file is not None:
        try:
            df_old = pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv") else pd.read_excel(uploaded_file)
            existing_ids, stats_norm = load_existing_ids(
                yt, df_old, treat_bare_names=treat_bare_names,
                do_normalize=normalize_handles, max_to_normalize=int(max_to_normalize)
            )
            st.info(
                f"üìÇ –ò—Å–∫–ª—é—á–∞–µ–º {len(existing_ids)} –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ –±–∞–∑—ã "
                f"(channel_id: {stats_norm.get('taken_channel_id',0)}, "
                f"–∏–∑ URL: {stats_norm.get('extracted_from_url',0)}, "
                f"–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: {stats_norm.get('normalized',0)}, "
                f"–Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {stats_norm.get('unresolved',0)})"
            )
        except Exception as e:
            st.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –±–∞–∑—É: {e}")

    # 1) –ø–æ –∫–ª—é—á–∞–º
    all_channels = {}
    prog = st.progress(0); stat = st.empty()
    for i, kw in enumerate(keywords, start=1):
        stat.write(f"üîé –ü–æ–∏—Å–∫ ¬´{kw}¬ª ({i}/{len(keywords)}) ‚Ä¶")
        found = search_channels_by_keyword(yt, kw, max_pages_per_keyword, max_channels_per_keyword)
        for cid, title in found.items():
            all_channels.setdefault(cid, title)
        prog.progress(int(i / len(keywords) * 25))

    if not all_channels:
        st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π –¥—Ä—É–≥–∏–µ –∫–ª—é—á–∏/–±–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü.")
        st.stop()

    # —ç–∫–æ–Ω–æ–º–∏—è –∫–≤–æ—Ç—ã ‚Äî –≤—ã–∫–∏–Ω—É—Ç—å —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –¥–æ stats
    before = len(all_channels)
    all_channels = {cid: t for cid, t in all_channels.items() if cid not in existing_ids}
    st.caption(f"üßπ –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∏—Å–∫–ª—é—á–µ–Ω–æ {before - len(all_channels)} –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ –±–∞–∑—ã")

    # 2) –±–∞—Ç—á-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stat.write(f"üì¶ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–Ω–∞–ª–æ–≤ ({len(all_channels)}) ‚Ä¶")
    stats_map = fetch_channels_stats(yt, list(all_channels.keys()))
    prog.progress(65)

    # 3) —Ñ–∏–ª—å—Ç—Ä –ø–æ–¥–ø–∏—Å—á–∏–∫–∏/total
    base_pass = [cid for cid, s in stats_map.items()
                 if s["uploads_playlist_id"]
                 and (min_subs <= s["subs"] <= max_subs)
                 and (s["total_views"] >= min_views_total)]
    if not base_pass:
        st.warning("–ù–∏–∫—Ç–æ –Ω–µ –ø—Ä–æ—à—ë–ª —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º/–ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º.")
        st.stop()

    # safety: —É–±—Ä–∞—Ç—å —Å–Ω–æ–≤–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö (–µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ª—É—á–∏–ª–∞—Å—å –ø–æ–∑–∂–µ)
    base_pass = [cid for cid in base_pass if cid not in existing_ids]

    # 4) —Å—Ä–µ–¥–Ω–∏–µ –∑–∞ –ø–µ—Ä–∏–æ–¥
    rows = []
    for i, cid in enumerate(base_pass, start=1):
        title = all_channels.get(cid, cid)
        s = stats_map[cid]
        stat.write(f"‚è±Ô∏è {i}/{len(base_pass)} ‚Ä¢ –°—Ä–µ–¥–Ω–∏–µ –∑–∞ {period_days} –¥–Ω.: {title}")
        avg, count = get_avg_views_for_period(yt, s["uploads_playlist_id"], period_days, max_recent_uploads_fetch)
        if avg is None or avg < min_avg_views_period:
            prog.progress(65 + int(35 * i / len(base_pass)))
            continue
        rows.append({
            "–ö–∞–Ω–∞–ª": title,
            "–ü–æ–¥–ø–∏—Å—á–∏–∫–∏": s["subs"],
            "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã_total": s["total_views"],
            "–°—Ä–µ–¥–Ω–∏–µ_–∑–∞_–¥–Ω–∏": avg,
            "–í–∏–¥–µ–æ_–∑–∞_–¥–Ω–∏": count,
            "–°—Ç—Ä–∞–Ω–∞": s["country"],
            "–°—Å—ã–ª–∫–∞": f"https://www.youtube.com/channel/{cid}",
            "channel_id": cid
        })
        prog.progress(65 + int(35 * i / len(base_pass)))

    stat.empty(); prog.empty()

    if rows:
        df = pd.DataFrame(rows).drop_duplicates(subset=["channel_id"])
        df = df.sort_values(by=["–°—Ä–µ–¥–Ω–∏–µ_–∑–∞_–¥–Ω–∏", "–ü–æ–¥–ø–∏—Å—á–∏–∫–∏"], ascending=[False, False])
        st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(df)} –Ω–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤")
        st.dataframe(df.drop(columns=["channel_id"]), use_container_width=True)

        xlsx = "bloggers.xlsx"
        df.to_excel(xlsx, index=False)
        with open(xlsx, "rb") as f:
            st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel", f, file_name="bloggers.xlsx")
    else:
        st.warning("–ü–æ –∑–∞–¥–∞–Ω–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –û—Å–ª–∞–±—å —Ñ–∏–ª—å—Ç—Ä—ã –∏–ª–∏ —É–≤–µ–ª–∏—á—å –ø–µ—Ä–∏–æ–¥.")
