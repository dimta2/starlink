# StarLinker v. 1.2
# a tool for searching YouTube influencers
# (c) StarPets 2025

import os
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Tuple, Optional, Iterable

import streamlit as st
import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from dotenv import load_dotenv

# =================== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===================
load_dotenv()
API_KEY = os.getenv("YOUTUBE_API_KEY")

st.set_page_config(page_title="StarLinker", page_icon="üîé", layout="wide")
st.title("üîé StarLinker ‚Äî –ü–æ–∏—Å–∫ –±–ª–æ–≥–µ—Ä–æ–≤ –Ω–∞ YouTube (v1.2)")

# -------- –ü–∞–Ω–µ–ª—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ --------
with st.sidebar:
    st.header("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞")
    max_pages_per_keyword = st.number_input(
        "–ú–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∫–ª—é—á (–ø–æ 50 –≤–∏–¥–µ–æ)",
        min_value=1, max_value=10, value=3, step=1,
        help="–ü–∞–≥–∏–Ω–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ YouTube. 1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞ = –¥–æ 50 –≤–∏–¥–µ–æ."
    )
    max_channels_per_keyword = st.number_input(
        "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ –∫–ª—é—á",
        min_value=10, max_value=1000, value=200, step=10,
        help="–ß—Ç–æ–±—ã –Ω–µ —Å–∂–∏–≥–∞—Ç—å –∫–≤–æ—Ç—É, –º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å —á–∏—Å–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ –∫–ª—é—á."
    )
    max_recent_uploads_fetch = st.number_input(
        "–ú–∞–∫—Å. –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –Ω–∞ –∫–∞–Ω–∞–ª (–¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ)",
        min_value=20, max_value=500, value=150, step=10,
        help="–°–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –≤–∏–¥–µ–æ —Å–º–æ—Ç—Ä–µ—Ç—å –∏–∑ –ø–ª–µ–π–ª–∏—Å—Ç–∞ –∑–∞–≥—Ä—É–∑–æ–∫ –∫–∞–Ω–∞–ª–∞."
    )

    st.markdown("---")
    st.header("üìä –§–∏–ª—å—Ç—Ä—ã")
    min_subs = st.number_input("–ú–∏–Ω. –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤", value=1_000, step=500)
    max_subs = st.number_input("–ú–∞–∫—Å. –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤", value=500_000, step=1_000)
    min_views_total = st.number_input("–ú–∏–Ω. –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∫–∞–Ω–∞–ª–∞ (total)", value=10_000, step=1_000)

    period_days = st.number_input("–ü–µ—Ä–∏–æ–¥ (–¥–Ω–µ–π) –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ –≤–∏–¥–µ–æ", value=30, min_value=1, max_value=90)
    min_avg_views_period = st.number_input("–ú–∏–Ω. —Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥", value=2_000, step=100)

# -------- –í–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö --------
keywords_input = st.text_area("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ)")
keywords = [kw.strip() for kw in keywords_input.splitlines() if kw.strip()]

st.caption("‚ÑπÔ∏è –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: —É–≤–µ–ª–∏—á—å —á–∏—Å–ª–æ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∫–ª—é—á –∏ –ª–∏–º–∏—Ç –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞ –∫–ª—é—á, –µ—Å–ª–∏ –Ω–∞—Ö–æ–¥–æ–∫ –º–∞–ª–æ. "
           "–ü—Ä–∏ —Å–ª–∏—à–∫–æ–º —Å—Ç—Ä–æ–≥–∏—Ö —Ñ–∏–ª—å—Ç—Ä–∞—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±—É–¥–µ—Ç –º–µ–Ω—å—à–µ.")

# =================== –£–¢–ò–õ–ò–¢–´ ===================
def chunked(iterable: List[str], size: int) -> Iterable[List[str]]:
    for i in range(0, len(iterable), size):
        yield iterable[i:i + size]

def iso_to_dt(s: str) -> datetime:
    return datetime.fromisoformat(s.replace("Z", "+00:00"))

def build_client(api_key: str):
    return build("youtube", "v3", developerKey=api_key)

# -------- –ü–æ–∏—Å–∫ –∫–∞–Ω–∞–ª–æ–≤ –ø–æ –∫–ª—é—á—É (–ø–∞–≥–∏–Ω–∞—Ü–∏—è) --------
def search_channels_by_keyword(youtube, keyword: str, max_pages: int, max_channels: int) -> Dict[str, str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {channel_id: channel_title} –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –∫–ª—é—á–∞, –ø—Ä–æ—Ö–æ–¥—è –¥–æ max_pages —Å—Ç—Ä–∞–Ω–∏—Ü."""
    channel_map: Dict[str, str] = {}
    page_token: Optional[str] = None
    pages = 0

    while True:
        try:
            resp = youtube.search().list(
                q=keyword,
                part="snippet",
                type="video",
                maxResults=50,
                pageToken=page_token
            ).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á—É ¬´{keyword}¬ª: {e}")
            break

        for it in resp.get("items", []):
            ch_id = it["snippet"]["channelId"]
            ch_title = it["snippet"]["channelTitle"]
            if ch_id not in channel_map:
                channel_map[ch_id] = ch_title
                if len(channel_map) >= max_channels:
                    return channel_map

        page_token = resp.get("nextPageToken")
        pages += 1
        if not page_token or pages >= max_pages:
            break

    return channel_map

# -------- –ë–∞—Ç—á-–∑–∞–ø—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤ --------
def fetch_channels_stats(youtube, channel_ids: List[str]) -> Dict[str, Tuple[int, int, str, str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å:
      channel_id -> (subs, total_views, country, uploads_playlist_id)
    """
    out: Dict[str, Tuple[int, int, str, str]] = {}
    for batch in chunked(channel_ids, 50):
        try:
            res = youtube.channels().list(
                part="statistics,snippet,contentDetails",
                id=",".join(batch)
            ).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤: {e}")
            continue

        for item in res.get("items", []):
            ch_id = item["id"]
            stats = item.get("statistics", {})
            snip = item.get("snippet", {})
            cdet = item.get("contentDetails", {})
            subs = int(stats.get("subscriberCount", 0) or 0)
            total_views = int(stats.get("viewCount", 0) or 0)
            country = snip.get("country", "N/A")
            uploads_pid = cdet.get("relatedPlaylists", {}).get("uploads", "")
            if uploads_pid:
                out[ch_id] = (subs, total_views, country, uploads_pid)
    return out

# -------- –ò—Ç–µ—Ä–∞—Ü–∏—è –ø–æ –∑–∞–≥—Ä—É–∑–∫–∞–º –∑–∞ –ø–µ—Ä–∏–æ–¥ --------
def iter_recent_upload_video_ids(
    youtube, uploads_playlist_id: str, since_dt: datetime, max_fetch: int
) -> Iterable[str]:
    fetched = 0
    page_token: Optional[str] = None
    while True:
        try:
            pl = youtube.playlistItems().list(
                part="contentDetails",
                playlistId=uploads_playlist_id,
                maxResults=50,
                pageToken=page_token
            ).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–ª–µ–π–ª–∏—Å—Ç–∞ –∑–∞–≥—Ä—É–∑–æ–∫: {e}")
            return

        for it in pl.get("items", []):
            fetched += 1
            pub = iso_to_dt(it["contentDetails"]["videoPublishedAt"])
            if pub >= since_dt:
                yield it["contentDetails"]["videoId"]
            else:
                # –¢–∞–∫ –∫–∞–∫ –ø–ª–µ–π–ª–∏—Å—Ç –∏–¥—ë—Ç –æ—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º ‚Äî –º–æ–∂–Ω–æ —Å—Ä–∞–∑—É –∑–∞–∫–æ–Ω—á–∏—Ç—å
                return
            if fetched >= max_fetch:
                return

        page_token = pl.get("nextPageToken")
        if not page_token:
            return

# -------- –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥ --------
def get_avg_views_for_period(
    youtube, uploads_playlist_id: str, days: int, max_fetch: int
) -> Tuple[Optional[int], int]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (avg_views_or_None, count_videos_in_period).
    None ‚Äî –µ—Å–ª–∏ –∑–∞ –ø–µ—Ä–∏–æ–¥ –Ω–µ—Ç –≤–∏–¥–µ–æ.
    """
    since_dt = datetime.now(timezone.utc) - timedelta(days=days)
    vids = list(iter_recent_upload_video_ids(youtube, uploads_playlist_id, since_dt, max_fetch=max_fetch))
    if not vids:
        return None, 0

    total_views = 0
    counted = 0
    for batch in chunked(vids, 50):
        try:
            vres = youtube.videos().list(part="statistics", id=",".join(batch)).execute()
        except HttpError as e:
            st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤–∏–¥–µ–æ: {e}")
            continue
        for it in vres.get("items", []):
            vc = int(it.get("statistics", {}).get("viewCount", 0) or 0)
            total_views += vc
            counted += 1

    if counted == 0:
        return None, 0
    return total_views // counted, counted

# =================== –û–°–ù–û–í–ù–û–ô –ë–õ–û–ö ===================
if st.button("üîç –ù–∞–π—Ç–∏ –±–ª–æ–≥–µ—Ä–æ–≤"):
    if not API_KEY:
        st.error("‚ùå API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å —Ñ–∞–π–ª .env (YOUTUBE_API_KEY=...)")
    elif not keywords:
        st.error("‚ùå –í–≤–µ–¥–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ!")
    else:
        youtube = build_client(API_KEY)

        all_channels: Dict[str, str] = {}
        progress = st.progress(0)
        status = st.empty()

        # 1) –ü–æ–∏—Å–∫ –∫–∞–Ω–∞–ª–æ–≤ –ø–æ –∫–ª—é—á–∞–º (—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π)
        for idx, kw in enumerate(keywords, start=1):
            status.write(f"üîé –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á—É ¬´{kw}¬ª ({idx}/{len(keywords)}) ‚Ä¶")
            found = search_channels_by_keyword(
                youtube, kw,
                max_pages=int(max_pages_per_keyword),
                max_channels=int(max_channels_per_keyword),
            )
            # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ
            for ch_id, title in found.items():
                all_channels.setdefault(ch_id, title)

            progress.progress(int(idx / max(1, len(keywords)) * 33))  # –¥–æ ~33%

        if not all_channels:
            st.warning("üòï –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º. –ü–æ–ø—Ä–æ–±—É–π –¥—Ä—É–≥–∏–µ –∫–ª—é—á–∏ –∏–ª–∏ —É–≤–µ–ª–∏—á—å –ø–∞–≥–∏–Ω–∞—Ü–∏—é.")
            st.stop()

        # 2) –¢—è–Ω–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤ –±–∞—Ç—á–∞–º–∏
        status.write(f"üì¶ –ü–æ–ª—É—á–∞—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤ (–≤—Å–µ–≥–æ {len(all_channels)}) ‚Ä¶")
        stats_map = fetch_channels_stats(youtube, list(all_channels.keys()))
        progress.progress(60)

        # 3) –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –±–∞–∑–æ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º –∫–∞–Ω–∞–ª–∞
        base_pass_ids = []
        for ch_id, tpl in stats_map.items():
            subs, total_views, _country, _uploads = tpl
            if min_subs <= subs <= max_subs and total_views >= min_views_total:
                base_pass_ids.append(ch_id)

        if not base_pass_ids:
            st.warning("üòï –ù–∏ –æ–¥–∏–Ω –∫–∞–Ω–∞–ª –Ω–µ –ø—Ä–æ—à—ë–ª –±–∞–∑–æ–≤—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º/total –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º.")
            st.stop()

        # 4) –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥ –∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
        results: List[Dict] = []
        for i, ch_id in enumerate(base_pass_ids, start=1):
            title = all_channels.get(ch_id, ch_id)
            subs, total_views, country, uploads_pid = stats_map[ch_id]

            status.write(f"‚è±Ô∏è {i}/{len(base_pass_ids)} ‚Ä¢ –°—á–∏—Ç–∞—é —Å—Ä–µ–¥–Ω–∏–µ –∑–∞ {int(period_days)} –¥–Ω. –¥–ª—è: {title}")
            avg_views, n_vids = get_avg_views_for_period(
                youtube, uploads_pid, int(period_days), max_fetch=int(max_recent_uploads_fetch)
            )

            if avg_views is None or avg_views < int(min_avg_views_period):
                continue

            results.append({
                "–ö–∞–Ω–∞–ª": title,
                "–ü–æ–¥–ø–∏—Å—á–∏–∫–∏": subs,
                "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã (total)": total_views,
                f"–°—Ä–µ–¥–Ω–∏–µ –∑–∞ {int(period_days)} –¥–Ω.": avg_views,
                f"–í–∏–¥–µ–æ –∑–∞ {int(period_days)} –¥–Ω.": n_vids,
                "–°—Ç—Ä–∞–Ω–∞": country,
                "–°—Å—ã–ª–∫–∞": f"https://www.youtube.com/channel/{ch_id}"
            })

            # –æ–±–Ω–æ–≤–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å –æ—Ç 60% –∫ 100%
            progress.progress(60 + int(40 * i / max(1, len(base_pass_ids))))

        status.empty()
        progress.empty()

        # 5) –í—ã–≤–æ–¥ –∏ –≤—ã–≥—Ä—É–∑–∫–∞
        if results:
            df = pd.DataFrame(results)
            df.drop_duplicates(subset=["–°—Å—ã–ª–∫–∞"], inplace=True)
            sort_col = f"–°—Ä–µ–¥–Ω–∏–µ –∑–∞ {int(period_days)} –¥–Ω."
            df.sort_values(by=[sort_col, "–ü–æ–¥–ø–∏—Å—á–∏–∫–∏"], ascending=[False, False], inplace=True)

            st.success(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(df)} –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ {len(all_channels)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
            st.dataframe(df, use_container_width=True)

            excel_file = "bloggers.xlsx"
            df.to_excel(excel_file, index=False)
            with open(excel_file, "rb") as f:
                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å Excel",
                    data=f,
                    file_name="bloggers.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        else:
            st.warning("üòï –ü–æ –∑–∞–¥–∞–Ω–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –û—Å–ª–∞–±—å —Ñ–∏–ª—å—Ç—Ä—ã –∏–ª–∏ —É–≤–µ–ª–∏—á—å –ø–µ—Ä–∏–æ–¥/—Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞.")
